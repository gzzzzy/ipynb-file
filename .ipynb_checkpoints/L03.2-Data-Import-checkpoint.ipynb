{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import and storage\n",
    "Input and output typically falls into a few main categories: reading text files and other more efficient on-disk formats like pdf and docx files, interacting with net-work sources like web APIs, and loading data from databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writing a text file\n",
    "### Using `open()` to read and write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`open()` returns a file object, and is most commonly used with two arguments: `open(filename, mode)`. It is good practice to use the with keyword when dealing with file objects. The advantage is that the file is properly closed after its suite finishes.\n",
    "\n",
    "- `mode` can be 'r' for read\n",
    "- 'w' for only writing\n",
    "- 'a' opens the file for appending\n",
    "- 'r+' opens the file for both reading and writing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    从2009年淘宝商城的促销活动，到2016年天猫商城全天交易额超越1207亿，“双十一”经过八年的升温，已经逐渐成长能够影响世界的中国电子商务行业年度盛事。在风光的“双十一”背后，技术保障团队又有什么样鲜为人知的故事呢？4月12日，阿里巴巴技术团队在京召开了分享会，独家揭秘“双十一”背后的“故事”，同时向大家推荐了题为《尽在双11——阿里巴巴技术演进与超越》的互联网丛书。\n",
      "    作为迄今为止，阿里巴巴集团唯一官方出品、全面阐述双 11 八年以来在技术和商业上演进和创新历程的书籍，《尽在双11——阿里巴巴技术演进与超越》由电子工业出版社博文视点与阿里巴巴联合出版。本书从五个方面全面精炼生动地进行剖析，揭秘世界奇迹双11背后的技术演进与创新，内容涵盖了双 11 背景下阿里技术架构八年来的演进，如何确保稳定性这条双 11 生命线的安全和可靠，技术和商业交织发展的历程，无线和互动的持续创新与突破，以及对商家的赋能和生态的促进与繁荣。\n",
      "    “1秒钟同时完成12万笔支付、单日交易额定格在1207亿元、每秒同时创建17.5万笔订单……”在这一串惊人的数字背后，蕴藏着技术团队鲜为人知的故事，对于他们来说，每年双11最紧张的就是午夜零点难以想象的流量。会议现场，双十一技术团部大队长陈琴、双十一稳定性负责人丁宇、认知计算实验室负责人袁泉分别从各自负责的业务板块，向大家细致解读了“双十一”背后，技术团队所付出的艰辛。三位负责人纷纷表示，他们分享的内容都包含在《尽在双11——阿里巴巴技术演进与超越》一书中，是双11八年成长经验与技术创新的总结，也是阿里成长中摸索出的方法和方向的汇聚，更是诸多技术同学与技术大神的倾囊分享。\n",
      "    阿里巴巴集团CEO张勇，对该书给予了高度的肯定。他认为，“《尽在双11——阿里巴巴技术演进与超越》以双11为着眼点，从技术的角度，展示了阿里巴巴的演进、变革与发展，系统地阐述了阿里巴巴重要阶段的技术进步历程。进无止境，我们希望将我们的经验分享给更多人，并希望与大家一起共同探索未来。”\n",
      "    阿里巴巴集团CTO张剑锋也力荐《尽在双11——阿里巴巴技术演进与超越》一书。他说，“《尽在双11——阿里巴巴技术演进与超越》是对“双11”技术演进客观、翔实的还原，迄今为止还没有类似的其他书。”（周靖杰）\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/BABAnews.txt\", mode = 'r') as myfile: \n",
    "        print(myfile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re not using the with keyword, then you should call f.close() to close the file and immediately free up any system resources used by it. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    从2009年淘宝商城的促销活动，到2016年天猫商城全天交易额超越1207亿，“双十一”经过八年的升温，已经逐渐成长能够影响世界的中国电子商务行业年度盛事。在风光的“双十一”背后，技术保障团队又有什么样鲜为人知的故事呢？4月12日，阿里巴巴技术团队在京召开了分享会，独家揭秘“双十一”背后的“故事”，同时向大家推荐了题为《尽在双11——阿里巴巴技术演进与超越》的互联网丛书。\n",
      "    作为迄今为止，阿里巴巴集团唯一官方出品、全面阐述双 11 八年以来在技术和商业上演进和创新历程的书籍，《尽在双11——阿里巴巴技术演进与超越》由电子工业出版社博文视点与阿里巴巴联合出版。本书从五个方面全面精炼生动地进行剖析，揭秘世界奇迹双11背后的技术演进与创新，内容涵盖了双 11 背景下阿里技术架构八年来的演进，如何确保稳定性这条双 11 生命线的安全和可靠，技术和商业交织发展的历程，无线和互动的持续创新与突破，以及对商家的赋能和生态的促进与繁荣。\n",
      "    “1秒钟同时完成12万笔支付、单日交易额定格在1207亿元、每秒同时创建17.5万笔订单……”在这一串惊人的数字背后，蕴藏着技术团队鲜为人知的故事，对于他们来说，每年双11最紧张的就是午夜零点难以想象的流量。会议现场，双十一技术团部大队长陈琴、双十一稳定性负责人丁宇、认知计算实验室负责人袁泉分别从各自负责的业务板块，向大家细致解读了“双十一”背后，技术团队所付出的艰辛。三位负责人纷纷表示，他们分享的内容都包含在《尽在双11——阿里巴巴技术演进与超越》一书中，是双11八年成长经验与技术创新的总结，也是阿里成长中摸索出的方法和方向的汇聚，更是诸多技术同学与技术大神的倾囊分享。\n",
      "    阿里巴巴集团CEO张勇，对该书给予了高度的肯定。他认为，“《尽在双11——阿里巴巴技术演进与超越》以双11为着眼点，从技术的角度，展示了阿里巴巴的演进、变革与发展，系统地阐述了阿里巴巴重要阶段的技术进步历程。进无止境，我们希望将我们的经验分享给更多人，并希望与大家一起共同探索未来。”\n",
      "    阿里巴巴集团CTO张剑锋也力荐《尽在双11——阿里巴巴技术演进与超越》一书。他说，“《尽在双11——阿里巴巴技术演进与超越》是对“双11”技术演进客观、翔实的还原，迄今为止还没有类似的其他书。”（周靖杰）\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "972"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile = open(\"data/BABAnews.txt\", mode = 'r')\n",
    "text = myfile.read()\n",
    "myfile.close()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-f51008fa62ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "myfile.read() # can be read after the file being closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the contents of a file, we use `f.read(size)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    从2009年淘宝商城的促销活动，到2016年天猫商城全天交易额超越1207亿，“双十一”经过八年的升温，已经逐渐成长能够影响世界的中国电子商务行业年度盛事。在风光的“双十一”背后，技术保障团队又有什么样鲜为人知的故事呢？4月12日，阿里巴巴技术团队在京召开了分享会，独家揭秘“双十一”背后的“故事”，同时向大家推荐了题为《尽在双11——阿里巴巴技术演进与超越》的互联网丛书。\\n    作为迄今为止，阿里巴巴集团唯一官方出品、全面阐述双 11 八年以来在技术和商业上演进和创新历程的书籍，《尽在双11——阿里巴巴技术演进与超越》由电子工业出版社博文视点与阿里巴巴联合出版。本书从五个方面全面精炼生动地进行剖析，揭秘世界奇迹双11背后的技术演进与创新，内容涵盖了双 11 背景下阿里技术架构八年来的演进，如何确保稳定性这条双 11 生命线的安全和可靠，技术和商业交织发展的历程，无线和互动的持续创新与突破，以及对商家的赋能和生态的促进与繁荣。\\n    “1秒钟同时完成12万笔支付、单日交易额定格在1207亿元、每秒同时创建17.5万笔订单……”在这一串惊人的数字背后，蕴藏着技术团队鲜为人知的故事，对于他们来说，每年双11最紧张的就是午夜零点难以想象的流量。会议现场，双十一技术团部大队长陈琴、双十一稳定性负责人丁宇、认知计算实验室负责人袁泉分别从各自负责的业务板块，向大家细致解读了“双十一”背后，技术团队所付出的艰辛。三位负责人纷纷表示，他们分享的内容都包含在《尽在双11——阿里巴巴技术演进与超越》一书中，是双11八年成长经验与技术创新的总结，也是阿里成长中摸索出的方法和方向的汇聚，更是诸多技术同学与技术大神的倾囊分享。\\n    阿里巴巴集团CEO张勇，对该书给予了高度的肯定。他认为，“《尽在双11——阿里巴巴技术演进与超越》以双11为着眼点，从技术的角度，展示了阿里巴巴的演进、变革与发展，系统地阐述了阿里巴巴重要阶段的技术进步历程。进无止境，我们希望将我们的经验分享给更多人，并希望与大家一起共同探索未来。”\\n    阿里巴巴集团CTO张剑锋也力荐《尽在双11——阿里巴巴技术演进与超越》一书。他说，“《尽在双11——阿里巴巴技术演进与超越》是对“双11”技术演进客观、翔实的还原，迄今为止还没有类似的其他书。”（周靖杰）\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile = open(\"data/BABAnews.txt\", mode = 'r')\n",
    "myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile.read()\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f.readline()` reads a single line from the file; a newline character (\\n) is left at the end of the string, and is only omitted on the last line of the file if the file doesn’t end in a newline. This makes the return value unambiguous; if `f.readline()` returns an empty string, the end of the file has been reached, while a blank line is represented by '\\n', a string containing only a single newline. \n",
    "\n",
    "If you want to read all the lines of a file in a list you can also use `list(f)` or `f.readlines()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    从2009年淘宝商城的促销活动，到2016年天猫商城全天交易额超越1207亿，“双十一”经过八年的升温，已经逐渐成长能够影响世界的中国电子商务行业年度盛事。在风光的“双十一”背后，技术保障团队又有什么样鲜为人知的故事呢？4月12日，阿里巴巴技术团队在京召开了分享会，独家揭秘“双十一”背后的“故事”，同时向大家推荐了题为《尽在双11——阿里巴巴技术演进与超越》的互联网丛书。\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile = open(\"data/BABAnews.txt\", mode = 'r')\n",
    "myfile.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    作为迄今为止，阿里巴巴集团唯一官方出品、全面阐述双 11 八年以来在技术和商业上演进和创新历程的书籍，《尽在双11——阿里巴巴技术演进与超越》由电子工业出版社博文视点与阿里巴巴联合出版。本书从五个方面全面精炼生动地进行剖析，揭秘世界奇迹双11背后的技术演进与创新，内容涵盖了双 11 背景下阿里技术架构八年来的演进，如何确保稳定性这条双 11 生命线的安全和可靠，技术和商业交织发展的历程，无线和互动的持续创新与突破，以及对商家的赋能和生态的促进与繁荣。\\n',\n",
       " '    “1秒钟同时完成12万笔支付、单日交易额定格在1207亿元、每秒同时创建17.5万笔订单……”在这一串惊人的数字背后，蕴藏着技术团队鲜为人知的故事，对于他们来说，每年双11最紧张的就是午夜零点难以想象的流量。会议现场，双十一技术团部大队长陈琴、双十一稳定性负责人丁宇、认知计算实验室负责人袁泉分别从各自负责的业务板块，向大家细致解读了“双十一”背后，技术团队所付出的艰辛。三位负责人纷纷表示，他们分享的内容都包含在《尽在双11——阿里巴巴技术演进与超越》一书中，是双11八年成长经验与技术创新的总结，也是阿里成长中摸索出的方法和方向的汇聚，更是诸多技术同学与技术大神的倾囊分享。\\n',\n",
       " '    阿里巴巴集团CEO张勇，对该书给予了高度的肯定。他认为，“《尽在双11——阿里巴巴技术演进与超越》以双11为着眼点，从技术的角度，展示了阿里巴巴的演进、变革与发展，系统地阐述了阿里巴巴重要阶段的技术进步历程。进无止境，我们希望将我们的经验分享给更多人，并希望与大家一起共同探索未来。”\\n',\n",
       " '    阿里巴巴集团CTO张剑锋也力荐《尽在双11——阿里巴巴技术演进与超越》一书。他说，“《尽在双11——阿里巴巴技术演进与超越》是对“双11”技术演进客观、翔实的还原，迄今为止还没有类似的其他书。”（周靖杰）\\n']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Both `read()` and `readlines()` come with the concept of a cursor. After either command is executed, the cursor moves to the end of the file, leaving nothing more to read in. Therefore, once a file content has been read in, another attempt to read from the file object will produce an empty data object. If for some reason you must read the file content again, you must close and re-open the file. \n",
    "\n",
    "Lastly, rather than loading the entire file content into memory, you can iterate through the file object line by line using the `for` loop. This method is more memory-efficient and therefore recommended when dealing with a very large file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,Class,Sex,Age,Survived,Freq\n",
      "1,1st,Male,Child,No,0\n",
      "2,2nd,Male,Child,No,0\n",
      "3,3rd,Male,Child,No,35\n",
      "4,Crew,Male,Child,No,0\n",
      "5,1st,Female,Child,No,0\n",
      "6,2nd,Female,Child,No,0\n",
      "7,3rd,Female,Child,No,17\n",
      "8,Crew,Female,Child,No,0\n",
      "9,1st,Male,Adult,No,118\n",
      "10,2nd,Male,Adult,No,154\n",
      "11,3rd,Male,Adult,No,387\n",
      "12,Crew,Male,Adult,No,670\n",
      "13,1st,Female,Adult,No,4\n",
      "14,2nd,Female,Adult,No,13\n",
      "15,3rd,Female,Adult,No,89\n",
      "16,Crew,Female,Adult,No,3\n",
      "17,1st,Male,Child,Yes,5\n",
      "18,2nd,Male,Child,Yes,11\n",
      "19,3rd,Male,Child,Yes,13\n",
      "20,Crew,Male,Child,Yes,0\n",
      "21,1st,Female,Child,Yes,1\n",
      "22,2nd,Female,Child,Yes,13\n",
      "23,3rd,Female,Child,Yes,14\n",
      "24,Crew,Female,Child,Yes,0\n",
      "25,1st,Male,Adult,Yes,57\n",
      "26,2nd,Male,Adult,Yes,14\n",
      "27,3rd,Male,Adult,Yes,75\n",
      "28,Crew,Male,Adult,Yes,192\n",
      "29,1st,Female,Adult,Yes,140\n",
      "30,2nd,Female,Adult,Yes,80\n",
      "31,3rd,Female,Adult,Yes,76\n",
      "32,Crew,Female,Adult,Yes,20"
     ]
    }
   ],
   "source": [
    "with open('data/Titanic.csv', 'r') as myfile:\n",
    "    for line in myfile:\n",
    "        print(line, end ='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing methods also come in a pair: `write()` and `writelines()`. Like the corresponding reading methods, `write()` handles a single string, while `writelines()` handles a list of strings. \n",
    "\n",
    "Below, `write()` writes a single string each time to the designated output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/testfile.txt', 'w') \n",
    "file.write('Hello World \\n') \n",
    "file.write('and this is another line.') \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This time, we have `tobuy`, a list of strings, which `writelines()` writes out at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tobuy = ['milk\\n', 'butter\\n', 'coffee beans\\n', 'arugula\\n']\n",
    "file = open('data/grocerylist.txt', 'w')\n",
    "file.writelines(tobuy) # writelines(list)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only the string type can be written.** Writing methods only works with strings: `write()` takes a single string, and `writelines()` takes a list which contains strings only. Non-string type data must be first coerced into the string type by using the `str()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-417bbd7e9647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/math.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pi's value is \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# trying to write float, doesn't work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not float"
     ]
    }
   ],
   "source": [
    "pi = 3.141592\n",
    "fout = open('data/math.txt', 'w')\n",
    "fout.write(\"Pi's value is \")\n",
    "fout.write(pi) # trying to write float, doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout.write(str(pi))\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ways to read text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use other modules to read text files. For example, we can use **numpy** to read *txt* file.  **csv** and **pandas** can be used to read *csv* files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.    1.  200.1]\n",
      " [  2.    2.  199.5]\n",
      " [  3.    3.  199.4]\n",
      " [  4.    4.  198.9]\n",
      " [  5.    5.  199. ]\n",
      " [  6.    6.  200.2]\n",
      " [  7.    7.  198.6]\n",
      " [  8.    8.  200. ]\n",
      " [  9.    9.  200.3]\n",
      " [ 10.   10.  201.2]\n",
      " [ 11.   11.  201.6]\n",
      " [ 12.   12.  201.5]\n",
      " [ 13.   13.  201.5]\n",
      " [ 14.   14.  203.5]\n",
      " [ 15.   15.  204.9]\n",
      " [ 16.   16.  207.1]\n",
      " [ 17.   17.  210.5]\n",
      " [ 18.   18.  210.5]\n",
      " [ 19.   19.  209.8]\n",
      " [ 20.   20.  208.8]\n",
      " [ 21.   21.  209.5]\n",
      " [ 22.   22.  213.2]\n",
      " [ 23.   23.  213.7]\n",
      " [ 24.   24.  215.1]\n",
      " [ 25.   25.  218.7]\n",
      " [ 26.   26.  219.8]\n",
      " [ 27.   27.  220.5]\n",
      " [ 28.   28.  223.8]\n",
      " [ 29.   29.  222.8]\n",
      " [ 30.   30.  223.8]\n",
      " [ 31.   31.  221.7]\n",
      " [ 32.   32.  222.3]\n",
      " [ 33.   33.  220.8]\n",
      " [ 34.   34.  219.4]\n",
      " [ 35.   35.  220.1]\n",
      " [ 36.   36.  220.6]\n",
      " [ 37.   37.  218.9]\n",
      " [ 38.   38.  217.8]\n",
      " [ 39.   39.  217.7]\n",
      " [ 40.   40.  215. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "data = np.loadtxt('data/BJsales.txt', skiprows = 1, delimiter='\\t')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID  Class  Sex  Age  Survived  Freq\n",
      "1  1st  Male  Child  No  0\n",
      "2  2nd  Male  Child  No  0\n",
      "3  3rd  Male  Child  No  35\n",
      "4  Crew  Male  Child  No  0\n",
      "5  1st  Female  Child  No  0\n",
      "6  2nd  Female  Child  No  0\n",
      "7  3rd  Female  Child  No  17\n",
      "8  Crew  Female  Child  No  0\n",
      "9  1st  Male  Adult  No  118\n",
      "10  2nd  Male  Adult  No  154\n",
      "11  3rd  Male  Adult  No  387\n",
      "12  Crew  Male  Adult  No  670\n",
      "13  1st  Female  Adult  No  4\n",
      "14  2nd  Female  Adult  No  13\n",
      "15  3rd  Female  Adult  No  89\n",
      "16  Crew  Female  Adult  No  3\n",
      "17  1st  Male  Child  Yes  5\n",
      "18  2nd  Male  Child  Yes  11\n",
      "19  3rd  Male  Child  Yes  13\n",
      "20  Crew  Male  Child  Yes  0\n",
      "21  1st  Female  Child  Yes  1\n",
      "22  2nd  Female  Child  Yes  13\n",
      "23  3rd  Female  Child  Yes  14\n",
      "24  Crew  Female  Child  Yes  0\n",
      "25  1st  Male  Adult  Yes  57\n",
      "26  2nd  Male  Adult  Yes  14\n",
      "27  3rd  Male  Adult  Yes  75\n",
      "28  Crew  Male  Adult  Yes  192\n",
      "29  1st  Female  Adult  Yes  140\n",
      "30  2nd  Female  Adult  Yes  80\n",
      "31  3rd  Female  Adult  Yes  76\n",
      "32  Crew  Female  Adult  Yes  20\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('data/Titanic.csv') as csvfile:\n",
    "    titanicReader = csv.reader(csvfile)\n",
    "    for row in titanicReader:\n",
    "        print('  '.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pandas` I/O API is a set of top level `reader` functions accessed like `read_csv()` that generally return a pandas object. These functions includes\n",
    "\n",
    "    read_excel\n",
    "    read_hdf\n",
    "    read_sql\n",
    "    read_json\n",
    "    read_msgpack (experimental)\n",
    "    read_html\n",
    "    read_gbq (experimental)\n",
    "    read_stata\n",
    "    read_sas\n",
    "    read_clipboard\n",
    "    read_pickle\n",
    "    \n",
    "See [pandas IO tools](http://pandas.pydata.org/pandas-docs/stable/io.html) for detailed explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1st</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Crew</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1st</td>\n",
       "      <td>Female</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Class     Sex    Age Survived  Freq\n",
       "0   1   1st    Male  Child       No     0\n",
       "1   2   2nd    Male  Child       No     0\n",
       "2   3   3rd    Male  Child       No    35\n",
       "3   4  Crew    Male  Child       No     0\n",
       "4   5   1st  Female  Child       No     0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "titanicData = pd.read_csv('data/Titanic.csv') \n",
    "titanicData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also have used `read_table()` and specifying the delimiter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1st</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Crew</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1st</td>\n",
       "      <td>Female</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Class     Sex    Age Survived  Freq\n",
       "0   1   1st    Male  Child       No     0\n",
       "1   2   2nd    Male  Child       No     0\n",
       "2   3   3rd    Male  Child       No    35\n",
       "3   4  Crew    Male  Child       No     0\n",
       "4   5   1st  Female  Child       No     0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanicData = pd.read_table('data/Titanic.csv', sep=',')\n",
    "titanicData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A file will not always have a header row. To read this in, you have a couple of options. You can allow pandas to assign default column names, or you can specify names yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>200.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>199.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>198.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1      2\n",
       "0  1  1  200.1\n",
       "1  2  2  199.5\n",
       "2  3  3  199.4\n",
       "3  4  4  198.9\n",
       "4  5  5  199.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales = pd.read_csv('data/BJsales.csv', header = None)\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>200.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>199.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>198.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Time  Value\n",
       "0   1     1  200.1\n",
       "1   2     2  199.5\n",
       "2   3     3  199.4\n",
       "3   4     4  198.9\n",
       "4   5     5  199.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales = pd.read_csv('data/BJsales.csv', names=['ID', 'Time', 'Value'])\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can also be exported to delimited format. Let’s consider one of the CSV files read above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.to_csv('data/sales.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a Microsoft Excel file using `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excel files are a huge part of any business operation and it becomes imperative that you learn exactly how to import these into python for data analysis. In order to do this we can use the code snippet shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HairEyeColor', 'Nile']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Hair</th>\n",
       "      <th>Eye</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Male</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Male</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Red</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Male</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Blond</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Black</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Male</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   Hair    Eye   Sex  Freq\n",
       "0   1  Black  Brown  Male    32\n",
       "1   2  Brown  Brown  Male    53\n",
       "2   3    Red  Brown  Male    10\n",
       "3   4  Blond  Brown  Male     3\n",
       "4   5  Black   Blue  Male    11"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = 'data/example.xlsx'\n",
    "myfile = pd.ExcelFile(file) \n",
    "print(myfile.sheet_names) #printing out all the sheet names in the excel file\n",
    "dataframe = myfile.parse('HairEyeColor') #extracting data from the first sheet as a dataframe\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a pdf file using `PyPDF2`\n",
    "\n",
    "`PyPDF2` is a python library built as a PDF toolkit. It is capable of:\n",
    "\n",
    "- Extracting document information (title, author, …)\n",
    "- Splitting documents page by page\n",
    "- Merging documents page by page\n",
    "- Cropping pages\n",
    "- Merging multiple pages into a single page\n",
    "- Encrypting and decrypting PDF files\n",
    "- and more!\n",
    "\n",
    "Here we only demonstrate how to read a pdf file. Please find more details on https://pypi.org/project/PyPDF2/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELINGCONDITIONALDENSITIESUSINGFINITESMOOTH\n",
      "MIXTURES\n",
      "FENGLI,MATTIASVILLANI,ANDROBERTKOHN\n",
      "SverigesRiksbankWorkingPaperSeriesNo.245\n",
      "August2010\n",
      "Abstract.\n",
      "Smoothmixtures,i.e.mixturemodelswithcovariate-depen\n",
      "dentmixingweights,\n",
      "areveryusefulexiblemodelsforconditionaldensities.P\n",
      "reviousworkshowsthatusingtoo\n",
      "simplemixturecomponentsformodelingheteroscedastican\n",
      "d/orheavytaileddatacangive\n",
      "apoort,evenwithalargenumberofcomponents.Thispapere\n",
      "xploreshowwellasmooth\n",
      "mixtureofsymmetriccomponentscancaptureskeweddata.Si\n",
      "mulationsandapplicationson\n",
      "realdatashowthatincludingcovariate-dependentskewnes\n",
      "sinthecomponentscanleadto\n",
      "substantiallyimprovedperformanceonskeweddata,oftenu\n",
      "singamuchsmallernumberof\n",
      "components.Furthermore,variableselectioniseectivei\n",
      "nremovingunnecessarycovariatesin\n",
      "theskewness,whichmeansthatthereislittlelossinallowi\n",
      "ngforskewnessinthecomponents\n",
      "whenthedataareactuallysymmetric.Wealsointroducesmoo\n",
      "thmixturesofgammaand\n",
      "log-normalcomponentstomodelpositively-valuedrespons\n",
      "evariables.\n",
      "Keywords\n",
      ":Bayesianinference,MarkovchainMonteCarlo,MixtureofE\n",
      "xperts,Variable\n",
      "selection\n",
      "Li:DepartmentofStatistics,StockholmUniversity,SE-10\n",
      "691Stockholm,Sweden.Villani:Research\n",
      "Division,SverigesRiksbankandDepartmentofStatistics,\n",
      "StockholmUniversity.Kohn:AustralianSchool\n",
      "ofBusiness,UniversityofNewSouthWales,UNSW,Sydney205\n",
      "2,Australia.Theviewsexpressedinthis\n",
      "paperaresolelytheresponsibilityoftheauthorsandshoul\n",
      "dnotbeinterpretedasreectingtheviewsofthe\n",
      "ExecutiveBoardofSverigesRiksbank.\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "pdfFileObj = open('data/Li2011Wiley.pdf', 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "pdfReader.numPages\n",
    "pageObj = pdfReader.getPage(0)\n",
    "print(pageObj.extractText())\n",
    "pdfFileObj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a word file using `docx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "MODELING CONDITIONAL DENSITIES USING FINITE SMOOTH MIXTURES\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "doc = docx.Document('data/Li2011Wiley.docx')\n",
    "print(len(doc.paragraphs))\n",
    "print(doc.paragraphs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with HTML and Web APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many websites have public APIs providing data feeds via JSON or some other format. There are a number of ways to access these APIs from Python; one easy-to-use method that I recommend is the requests package (http://docs.python-requests.org). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['status', 't', 'set_cache_time', 'data'])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://sp0.baidu.com/8aQDcjqpAAV3otqbppnN2DJv/api.php?resource_id=6899&query=失信执行人名单&iname=中国银行'\n",
    "resp = requests.get(url)\n",
    "import json\n",
    "data = json.loads(resp.text, encoding='gb18030')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2010)历执字第00787号  山东  李光  济南市历下区人民法院  一、被告偿还原告15834.4元；二、被告更正原告的个人信用信息并上报于中国人民银行征信管理机构，为原告恢复银行信用记录，如因技术原因，中国人民银行征信管理机构无法更正信息，由被告中国银行股份有限公司济南分行与中国人民银行征信管理机构协商对原告黄金华的个人信用作特殊标记，以区别于其他不良信息，于本调解书生效之日起20日内执行；三、原、被告其余责任互不追究。  全部未履行  其他有履行能力而拒不履行生效法律文书确定义务  2015年08月25日  20100331  （2007）历民初字第3036号  济南市历下区人民法院  863152018\n",
      "(2015)龙泉执字第01421号  四川  林忠  龙泉驿区人民法院  中国银行股份有限公司成都航天城支行于本判决生效之日起向潘兴才支付存款损失94890元及利息。  全部未履行  违反财产报告制度,其他有履行能力而拒不履行生效法律文书确定义务  2015年08月10日  20150727  （2015）成民终字第2016号民事判决书  四川省成都市中级人民法院  782689497\n"
     ]
    }
   ],
   "source": [
    "result = data['data'][0]['result']\n",
    "for i in range(len(result)):\n",
    "        data = result[i]\n",
    "        caseCode = data['caseCode']\n",
    "        areaName = data['areaName']\n",
    "        businessEntity = data['businessEntity']\n",
    "        courtName = data['courtName']\n",
    "        duty = data['duty']\n",
    "        performance = data['performance']\n",
    "        disruptTypeName = data['disruptTypeName']\n",
    "        publishDate = data['publishDate']\n",
    "        regDate = data['regDate']\n",
    "        gistId = data['gistId']\n",
    "        gistUnit = data['gistUnit']\n",
    "        cardNum = data['cardNum']\n",
    "        print('  '.join([caseCode, areaName, businessEntity, courtName, duty, performance, disruptTypeName, publishDate, regDate, gistId, gistUnit, cardNum]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many applications data rarely comes from text files, that being a fairly inefficient way to store large amounts of data. SQL-based relational databases (such as SQL Server, PostgreSQL, and MySQL) are in wide use, and many alternative non-SQL (so-called NoSQL) databases have become quite popular. The choice of database is usually de- pendent on the performance, data integrity, and scalability needs of an application.\n",
    "\n",
    "Loading data from SQL into a DataFrame is fairly straightforward, and pandas has some functions to simplify the process. As an example, I’ll use an in-memory SQLite database using Python’s built-in sqlite3 driver. Here’s a short Python program that selects latitudes and longitudes from an SQLite database stored in a file called survey.db:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "connection = sqlite3.connect(\"survey.db\")\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT Site.lat, Site.long FROM Site;\")\n",
    "results = cursor.fetchall()\n",
    "for r in results:\n",
    "    print(r)\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program starts by importing the sqlite3 library. If we were connecting to MySQL, DB2, or some other database, we would import a different library, but all of them provide the same functions, so that the rest of our program does not have to change (at least, not much) if we switch from one database to another.\n",
    "\n",
    "Line 2 establishes a connection to the database. Since we’re using SQLite, all we need to specify is the name of the database file. Other systems may require us to provide a username and password as well. Line 3 then uses this connection to create a cursor. Just like the cursor in an editor, its role is to keep track of where we are in the database.\n",
    "\n",
    "On line 4, we use that cursor to ask the database to execute a query for us. The query is written in SQL, and passed to cursor.execute as a string. It’s our job to make sure that SQL is properly formatted; if it isn’t, or if something goes wrong when it is being executed, the database will report an error.\n",
    "\n",
    "The database returns the results of the query to us in response to the cursor.fetchall call on line 5. This result is a list with one entry for each record in the result set; if we loop over that list (line 6) and print those list entries (line 7), we can see that each one is a tuple with one element for each field we asked for.\n",
    "\n",
    "Finally, lines 8 and 9 close our cursor and our connection, since the database can only keep a limited number of these open at one time. Since establishing a connection takes time, though, we shouldn’t open a connection, do one operation, then close the connection, only to reopen it a few microseconds later to do another operation. Instead, it’s normal to create one connection that stays open for the lifetime of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "\n",
    "Read your five different types of your own files to Python from your hard disk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
